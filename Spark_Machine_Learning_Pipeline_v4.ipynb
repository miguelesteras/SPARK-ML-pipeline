{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark Machine Learning Pipeline - Task 2 (Big Data Module)\n",
    "\n",
    "Miguel Esteras & Alberto Ruiz Benitez de Lugo\n",
    "\n",
    "This coursework contains an implementation and application of Spark Machine Learning Pipelines. The piepeline is evaluated regarding preprocessing, parametrisation, and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section A) Choice of dataset and task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Santander products <-- DataSet\n",
    "\n",
    "The dataset used is called \"Santander Products\", part of the Kaggle competition with the same name. The reason of this choice has been the large amount of predictors and responses available within this data. This amount of possible variable combinations increases the complexity of the problem and therefore the selected predictor model should have a deep level of abstraction. Furthermore, this example will appropriately showcase the advantages of using Spark parallel computing for the analysis of large datasets. \n",
    "\n",
    "The chosen model, \"Random Forest\" (RF), is able to accommodate large numbers of features of diverse type. This method is currently state-of-the-art in many different Machine Learning fields, like computer vision. RF is expected to reach a good performance in both, classification and regression implementations. \n",
    "\n",
    "### Task\n",
    "\n",
    "The goal of the pipeline is to predict whether a financial product (a mortgage) will be purchased by a consumer, given the personal and financial data available for each customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section B) Machine Learning Pipeline in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Data set initial analysis and summary of pipeline task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 Summary of Pipeline\n",
    "\n",
    "- Load Data and first preprocessing (1.2)\n",
    "- Descriptive statistics (1.3)\n",
    "- Data Cleaning (1.4)\n",
    "- Machine learning pipeline Implementation using Random Forest (2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2. Loading data to RDD and first preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "type_dict = {'ncodpers':np.int32,\n",
    "            'ind_ahor_fin_ult1':np.uint8, 'ind_aval_fin_ult1':np.uint8, \n",
    "            'ind_cco_fin_ult1':np.uint8,'ind_cder_fin_ult1':np.uint8,\n",
    "            'ind_cno_fin_ult1':np.uint8,'ind_ctju_fin_ult1':np.uint8,'ind_ctma_fin_ult1':np.uint8,\n",
    "            'ind_ctop_fin_ult1':np.uint8,'ind_ctpp_fin_ult1':np.uint8,'ind_deco_fin_ult1':np.uint8,\n",
    "            'ind_deme_fin_ult1':np.uint8,'ind_dela_fin_ult1':np.uint8,'ind_ecue_fin_ult1':np.uint8,\n",
    "            'ind_fond_fin_ult1':np.uint8,'ind_hip_fin_ult1':np.uint8,'ind_plan_fin_ult1':np.uint8,\n",
    "            'ind_pres_fin_ult1':np.uint8,'ind_reca_fin_ult1':np.uint8,'ind_tjcr_fin_ult1':np.uint8,\n",
    "            'ind_valo_fin_ult1':np.uint8,'ind_viv_fin_ult1':np.uint8, 'ind_recibo_ult1':np.uint8 }\n",
    "\n",
    "# load data from server into dataframe (only loading the top 1,000,000 for demonstration purpose)\n",
    "df = pd.read_csv(\"/data/tempstore/santander-products/train_ver2.csv\",\n",
    "                 nrows = 1000000,\n",
    "                 dtype = type_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>indrel</th>\n",
       "      <th>indrel_1mes</th>\n",
       "      <th>tipodom</th>\n",
       "      <th>cod_prov</th>\n",
       "      <th>ind_actividad_cliente</th>\n",
       "      <th>renta</th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>ind_aval_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>989218.0</td>\n",
       "      <td>982266.000000</td>\n",
       "      <td>989218.000000</td>\n",
       "      <td>8.248170e+05</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>994598.000000</td>\n",
       "      <td>994598.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.905967e+05</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>1.109074</td>\n",
       "      <td>1.000085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.852131</td>\n",
       "      <td>0.564971</td>\n",
       "      <td>1.396462e+05</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.039378</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.071629</td>\n",
       "      <td>0.079543</td>\n",
       "      <td>0.166275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.044084e+05</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>3.267624</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.422924</td>\n",
       "      <td>0.495761</td>\n",
       "      <td>2.389858e+05</td>\n",
       "      <td>0.013303</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099410</td>\n",
       "      <td>0.119755</td>\n",
       "      <td>0.068112</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>0.248429</td>\n",
       "      <td>0.194493</td>\n",
       "      <td>0.080003</td>\n",
       "      <td>0.257873</td>\n",
       "      <td>0.270584</td>\n",
       "      <td>0.372327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.588900e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.202730e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.364110e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.157184e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.644760e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.066519e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.074511e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.634325e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.379131e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.889440e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ncodpers      ind_nuevo         indrel    indrel_1mes   tipodom  \\\n",
       "count  1.000000e+06  989218.000000  989218.000000  989218.000000  989218.0   \n",
       "mean   6.905967e+05       0.000489       1.109074       1.000085       1.0   \n",
       "std    4.044084e+05       0.022114       3.267624       0.012954       0.0   \n",
       "min    1.588900e+04       0.000000       1.000000       1.000000       1.0   \n",
       "25%    3.364110e+05       0.000000       1.000000       1.000000       1.0   \n",
       "50%    6.644760e+05       0.000000       1.000000       1.000000       1.0   \n",
       "75%    1.074511e+06       0.000000       1.000000       1.000000       1.0   \n",
       "max    1.379131e+06       1.000000      99.000000       3.000000       1.0   \n",
       "\n",
       "            cod_prov  ind_actividad_cliente         renta  ind_ahor_fin_ult1  \\\n",
       "count  982266.000000          989218.000000  8.248170e+05     1000000.000000   \n",
       "mean       26.852131               0.564971  1.396462e+05           0.000177   \n",
       "std        12.422924               0.495761  2.389858e+05           0.013303   \n",
       "min         1.000000               0.000000  1.202730e+03           0.000000   \n",
       "25%        18.000000               0.000000  7.157184e+04           0.000000   \n",
       "50%        28.000000               1.000000  1.066519e+05           0.000000   \n",
       "75%        33.000000               1.000000  1.634325e+05           0.000000   \n",
       "max        52.000000               1.000000  2.889440e+07           1.000000   \n",
       "\n",
       "       ind_aval_fin_ult1       ...         ind_hip_fin_ult1  \\\n",
       "count     1000000.000000       ...           1000000.000000   \n",
       "mean            0.000039       ...                 0.009982   \n",
       "std             0.006245       ...                 0.099410   \n",
       "min             0.000000       ...                 0.000000   \n",
       "25%             0.000000       ...                 0.000000   \n",
       "50%             0.000000       ...                 0.000000   \n",
       "75%             0.000000       ...                 0.000000   \n",
       "max             1.000000       ...                 1.000000   \n",
       "\n",
       "       ind_plan_fin_ult1  ind_pres_fin_ult1  ind_reca_fin_ult1  \\\n",
       "count     1000000.000000     1000000.000000     1000000.000000   \n",
       "mean            0.014553           0.004661           0.072581   \n",
       "std             0.119755           0.068112           0.259448   \n",
       "min             0.000000           0.000000           0.000000   \n",
       "25%             0.000000           0.000000           0.000000   \n",
       "50%             0.000000           0.000000           0.000000   \n",
       "75%             0.000000           0.000000           0.000000   \n",
       "max             1.000000           1.000000           1.000000   \n",
       "\n",
       "       ind_tjcr_fin_ult1  ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "count     1000000.000000     1000000.000000    1000000.000000   \n",
       "mean            0.066084           0.039378          0.006442   \n",
       "std             0.248429           0.194493          0.080003   \n",
       "min             0.000000           0.000000          0.000000   \n",
       "25%             0.000000           0.000000          0.000000   \n",
       "50%             0.000000           0.000000          0.000000   \n",
       "75%             0.000000           0.000000          0.000000   \n",
       "max             1.000000           1.000000          1.000000   \n",
       "\n",
       "       ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "count    994598.000000      994598.000000   1000000.000000  \n",
       "mean          0.071629           0.079543         0.166275  \n",
       "std           0.257873           0.270584         0.372327  \n",
       "min           0.000000           0.000000         0.000000  \n",
       "25%           0.000000           0.000000         0.000000  \n",
       "50%           0.000000           0.000000         0.000000  \n",
       "75%           0.000000           0.000000         0.000000  \n",
       "max           1.000000           1.000000         1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               1000000\n",
       "ncodpers                 1000000\n",
       "ind_empleado              989218\n",
       "pais_residencia           989218\n",
       "sexo                      989214\n",
       "age                      1000000\n",
       "fecha_alta                989218\n",
       "ind_nuevo                 989218\n",
       "antiguedad               1000000\n",
       "indrel                    989218\n",
       "ult_fec_cli_1t              1101\n",
       "indrel_1mes               989218\n",
       "tiprel_1mes               989218\n",
       "indresi                   989218\n",
       "indext                    989218\n",
       "conyuemp                     178\n",
       "canal_entrada             989139\n",
       "indfall                   989218\n",
       "tipodom                   989218\n",
       "cod_prov                  982266\n",
       "nomprov                   982266\n",
       "ind_actividad_cliente     989218\n",
       "renta                     824817\n",
       "segmento                  989105\n",
       "ind_ahor_fin_ult1        1000000\n",
       "ind_aval_fin_ult1        1000000\n",
       "ind_cco_fin_ult1         1000000\n",
       "ind_cder_fin_ult1        1000000\n",
       "ind_cno_fin_ult1         1000000\n",
       "ind_ctju_fin_ult1        1000000\n",
       "ind_ctma_fin_ult1        1000000\n",
       "ind_ctop_fin_ult1        1000000\n",
       "ind_ctpp_fin_ult1        1000000\n",
       "ind_deco_fin_ult1        1000000\n",
       "ind_deme_fin_ult1        1000000\n",
       "ind_dela_fin_ult1        1000000\n",
       "ind_ecue_fin_ult1        1000000\n",
       "ind_fond_fin_ult1        1000000\n",
       "ind_hip_fin_ult1         1000000\n",
       "ind_plan_fin_ult1        1000000\n",
       "ind_pres_fin_ult1        1000000\n",
       "ind_reca_fin_ult1        1000000\n",
       "ind_tjcr_fin_ult1        1000000\n",
       "ind_valo_fin_ult1        1000000\n",
       "ind_viv_fin_ult1         1000000\n",
       "ind_nomina_ult1           994598\n",
       "ind_nom_pens_ult1         994598\n",
       "ind_recibo_ult1          1000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only unique id\n",
    "unique_ids = pd.Series(df[\"ncodpers\"].unique())\n",
    "df = df[df.ncodpers.isin(unique_ids)]  \n",
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# eliminate mostly empty columns and redundant variables\n",
    "df.drop([\"tipodom\",\"cod_prov\", \"ult_fec_cli_1t\",\"conyuemp\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform to numeric and set missing values to nan\n",
    "df['age']=pd.to_numeric(df.age, errors='coerce')\n",
    "df['ind_nuevo']=pd.to_numeric(df.ind_nuevo, errors='coerce')\n",
    "df['antiguedad']=pd.to_numeric(df.antiguedad, errors='coerce')\n",
    "df['indrel']=pd.to_numeric(df.indrel, errors='coerce')\n",
    "df['renta']=pd.to_numeric(df.renta, errors='coerce')\n",
    "df['indrel_1mes']=pd.to_numeric(df.indrel_1mes, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove age outliers and nan from age variable\n",
    "df.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True) # replace outlier con mean\n",
    "df[\"age\"].fillna(df[\"age\"].mean(),inplace=True) # replace nan with mean\n",
    "df[\"age\"] = df[\"age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-01-28T00:00:00.000000000', '2015-02-28T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfor dates to datetime datatype\n",
    "df[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "df[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fill datetime missing values\n",
    "dates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\n",
    "median_date = int(np.median(dates.index.values))\n",
    "df.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado              True\n",
       "pais_residencia           True\n",
       "sexo                      True\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                 True\n",
       "antiguedad                True\n",
       "indrel                    True\n",
       "indrel_1mes               True\n",
       "tiprel_1mes               True\n",
       "indresi                   True\n",
       "indext                    True\n",
       "canal_entrada             True\n",
       "indfall                   True\n",
       "nomprov                   True\n",
       "ind_actividad_cliente     True\n",
       "renta                     True\n",
       "segmento                  True\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1           True\n",
       "ind_nom_pens_ult1         True\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values in target features with 0\n",
    "# target features = boolean indicator as to whether or not that product was owned that month\n",
    "df.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\n",
    "df.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace other missing values\n",
    "df.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1                   # new customers id '1'\n",
    "df.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\n",
    "df.loc[df.antiguedad <0, \"antiguedad\"] = 0                         # new customer antiguedad '0'\n",
    "df.loc[df.indrel.isnull(),\"indrel\"] = 1 \n",
    "df.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\n",
    "df[\"ind_actividad_cliente\"].median()                   # fill in customer activity missing\n",
    "df.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"      # known values for city of residence\n",
    "df.loc[df.indfall.isnull(),\"indfall\"] = \"N\"            # missing deceased index set to N\n",
    "df.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"    # customer status, if missing = active \n",
    "df.tiprel_1mes = df.tiprel_1mes.astype(\"category\")     # customer status as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Customer type normalization as categorical variable \n",
    "map_dict = { 1.0:\"1\", \"1.0\":\"1\", \"1\":\"1\", \"3.0\":\"3\", \"P\":\"P\", 3.0:\"3\", 2.0:\"2\", \"3\":\"3\", \"2.0\":\"2\", \"4.0\":\"4\", \"4\":\"4\", \"2\":\"2\"}\n",
    "df.indrel_1mes.fillna(\"P\",inplace=True)\n",
    "df.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "df.indrel_1mes = df.indrel_1mes.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove rows with any nan value left\n",
    "df = df.dropna(subset=['renta', 'segmento', 'canal_entrada', 'ind_empleado', \n",
    "                       'pais_residencia', 'indresi', 'indresi', 'sexo'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado             False\n",
       "pais_residencia          False\n",
       "sexo                     False\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                False\n",
       "antiguedad               False\n",
       "indrel                   False\n",
       "indrel_1mes              False\n",
       "tiprel_1mes              False\n",
       "indresi                  False\n",
       "indext                   False\n",
       "canal_entrada            False\n",
       "indfall                  False\n",
       "nomprov                  False\n",
       "ind_actividad_cliente    False\n",
       "renta                    False\n",
       "segmento                 False\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1          False\n",
       "ind_nom_pens_ult1        False\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all missing values are gone\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               824742\n",
       "ncodpers                 824742\n",
       "ind_empleado             824742\n",
       "pais_residencia          824742\n",
       "sexo                     824742\n",
       "age                      824742\n",
       "fecha_alta               824742\n",
       "ind_nuevo                824742\n",
       "antiguedad               824742\n",
       "indrel                   824742\n",
       "indrel_1mes              824742\n",
       "tiprel_1mes              824742\n",
       "indresi                  824742\n",
       "indext                   824742\n",
       "canal_entrada            824742\n",
       "indfall                  824742\n",
       "nomprov                  824742\n",
       "ind_actividad_cliente    824742\n",
       "renta                    824742\n",
       "segmento                 824742\n",
       "ind_ahor_fin_ult1        824742\n",
       "ind_aval_fin_ult1        824742\n",
       "ind_cco_fin_ult1         824742\n",
       "ind_cder_fin_ult1        824742\n",
       "ind_cno_fin_ult1         824742\n",
       "ind_ctju_fin_ult1        824742\n",
       "ind_ctma_fin_ult1        824742\n",
       "ind_ctop_fin_ult1        824742\n",
       "ind_ctpp_fin_ult1        824742\n",
       "ind_deco_fin_ult1        824742\n",
       "ind_deme_fin_ult1        824742\n",
       "ind_dela_fin_ult1        824742\n",
       "ind_ecue_fin_ult1        824742\n",
       "ind_fond_fin_ult1        824742\n",
       "ind_hip_fin_ult1         824742\n",
       "ind_plan_fin_ult1        824742\n",
       "ind_pres_fin_ult1        824742\n",
       "ind_reca_fin_ult1        824742\n",
       "ind_tjcr_fin_ult1        824742\n",
       "ind_valo_fin_ult1        824742\n",
       "ind_viv_fin_ult1         824742\n",
       "ind_nomina_ult1          824742\n",
       "ind_nom_pens_ult1        824742\n",
       "ind_recibo_ult1          824742\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # number of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Machine learning pipeline Implementation\n",
    "Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. Test that your pipeline it is correctly implemented and explain your choice of processing steps, learning algorithms, and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove any previous spark session and check df file type\n",
    "spark.stop()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark SQL dataframe \n",
    "## IMPORTANT!! - this cell usually takes time due to data volume!!!\n",
    "## IMPORTANT!! - Only run this cell once! (to run it again, you need to restart the kernel)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlCtx = SQLContext(sc) #print(sc)\n",
    "df_spark = sqlCtx.createDataFrame(df)\n",
    "type(df_spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define datatypes in dataframe\n",
    "\n",
    "df_spark = df_spark.select(df_spark.fecha_dato.cast(\"date\"),\n",
    "                                   df_spark.ncodpers.cast(\"float\"),\n",
    "                                   df_spark.ind_empleado.cast(\"string\"),\n",
    "                                   df_spark.pais_residencia.cast(\"string\"),\n",
    "                                   df_spark.sexo.cast(\"string\"),\n",
    "                                   df_spark.age.cast(\"float\"),\n",
    "                                   df_spark.fecha_alta.cast(\"date\"),\n",
    "                                   df_spark.ind_nuevo.cast(\"float\"),\n",
    "                                   df_spark.antiguedad.cast(\"float\"),\n",
    "                                   df_spark.indrel.cast(\"float\"),\n",
    "                                   df_spark.indrel_1mes.cast(\"float\"),\n",
    "                                   df_spark.tiprel_1mes.cast(\"string\"),\n",
    "                                   df_spark.indresi.cast(\"string\"),\n",
    "                                   df_spark.indext.cast(\"string\"),\n",
    "                                   df_spark.canal_entrada.cast(\"string\"),\n",
    "                                   df_spark.indfall.cast(\"string\"),\n",
    "                                   df_spark.nomprov.cast(\"string\"),\n",
    "                                   df_spark.ind_actividad_cliente.cast(\"float\"),\n",
    "                                   df_spark.renta.cast(\"float\"),\n",
    "                                   df_spark.segmento.cast(\"string\"),\n",
    "                                   df_spark.ind_ahor_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_aval_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cder_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_cno_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctju_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctma_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctop_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ctpp_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deco_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_deme_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_dela_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_ecue_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_fond_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_hip_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_plan_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_pres_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_reca_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_tjcr_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_valo_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_viv_fin_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nomina_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_nom_pens_ult1.cast(\"float\"),\n",
    "                                   df_spark.ind_recibo_ult1.cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha_dato: date (nullable = true)\n",
      " |-- ncodpers: float (nullable = true)\n",
      " |-- ind_empleado: string (nullable = true)\n",
      " |-- pais_residencia: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- fecha_alta: date (nullable = true)\n",
      " |-- ind_nuevo: float (nullable = true)\n",
      " |-- antiguedad: float (nullable = true)\n",
      " |-- indrel: float (nullable = true)\n",
      " |-- indrel_1mes: float (nullable = true)\n",
      " |-- tiprel_1mes: string (nullable = true)\n",
      " |-- indresi: string (nullable = true)\n",
      " |-- indext: string (nullable = true)\n",
      " |-- canal_entrada: string (nullable = true)\n",
      " |-- indfall: string (nullable = true)\n",
      " |-- nomprov: string (nullable = true)\n",
      " |-- ind_actividad_cliente: float (nullable = true)\n",
      " |-- renta: float (nullable = true)\n",
      " |-- segmento: string (nullable = true)\n",
      " |-- ind_ahor_fin_ult1: float (nullable = true)\n",
      " |-- ind_aval_fin_ult1: float (nullable = true)\n",
      " |-- ind_cco_fin_ult1: float (nullable = true)\n",
      " |-- ind_cder_fin_ult1: float (nullable = true)\n",
      " |-- ind_cno_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctju_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctma_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctop_fin_ult1: float (nullable = true)\n",
      " |-- ind_ctpp_fin_ult1: float (nullable = true)\n",
      " |-- ind_deco_fin_ult1: float (nullable = true)\n",
      " |-- ind_deme_fin_ult1: float (nullable = true)\n",
      " |-- ind_dela_fin_ult1: float (nullable = true)\n",
      " |-- ind_ecue_fin_ult1: float (nullable = true)\n",
      " |-- ind_fond_fin_ult1: float (nullable = true)\n",
      " |-- ind_hip_fin_ult1: float (nullable = true)\n",
      " |-- ind_plan_fin_ult1: float (nullable = true)\n",
      " |-- ind_pres_fin_ult1: float (nullable = true)\n",
      " |-- ind_reca_fin_ult1: float (nullable = true)\n",
      " |-- ind_tjcr_fin_ult1: float (nullable = true)\n",
      " |-- ind_valo_fin_ult1: float (nullable = true)\n",
      " |-- ind_viv_fin_ult1: float (nullable = true)\n",
      " |-- ind_nomina_ult1: float (nullable = true)\n",
      " |-- ind_nom_pens_ult1: float (nullable = true)\n",
      " |-- ind_recibo_ult1: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# code modified from Spark documentation at:\n",
    "# https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier\n",
    "# and DataBricks at:\n",
    "# https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "\n",
    "# imports dependencies for Random Forest pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# IMPORTANT - Define target label (for prediction) from target features. Target select = mortgage products\n",
    "labels = \"ind_hip_fin_ult1\"  \n",
    "\n",
    "# stages in the Pipeline\n",
    "stages = []\n",
    "    \n",
    "# define variables; categorical, countinuous and target features\n",
    "\n",
    "numericCols = [\"age\",\"antiguedad\",\"renta\"]\n",
    "\n",
    "categoricalColumns = [\"ind_empleado\",\"pais_residencia\",\"sexo\",\"ind_nuevo\",\"indrel\", \n",
    "                      \"indrel_1mes\",\"tiprel_1mes\", \"indresi\", \"indext\", \"canal_entrada\",\"nomprov\", \n",
    "                      \"ind_actividad_cliente\",\"segmento\"]\n",
    "\n",
    "targetsColumns = [\"ind_ahor_fin_ult1\", \"ind_aval_fin_ult1\",\n",
    "                        \"ind_cco_fin_ult1\", \"ind_cder_fin_ult1\", \"ind_cno_fin_ult1\",\n",
    "                        \"ind_ctma_fin_ult1\", \"ind_ctop_fin_ult1\",\n",
    "                        \"ind_ctpp_fin_ult1\", \"ind_deco_fin_ult1\", \"ind_deme_fin_ult1\", \n",
    "                        \"ind_dela_fin_ult1\", \"ind_ecue_fin_ult1\", \"ind_fond_fin_ult1\",\n",
    "                        \"ind_ctju_fin_ult1\", \"ind_plan_fin_ult1\", \"ind_pres_fin_ult1\",\n",
    "                        \"ind_reca_fin_ult1\", \"ind_tjcr_fin_ult1\", \"ind_valo_fin_ult1\", \n",
    "                        \"ind_viv_fin_ult1\", \"ind_nomina_ult1\", \"ind_nom_pens_ult1\",\"ind_recibo_ult1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol,\n",
    "                                  outputCol = categoricalCol + \"Index\") # Category Indexing with StringIndexer\n",
    "    stages += [stringIndexer]  # Add stages to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define categorical index columns \n",
    "categoricalColumnsIDX = [\"ind_empleadoIndex\",\"pais_residenciaIndex\",\"sexoIndex\",\n",
    "                         \"ind_nuevoIndex\",\"indrelIndex\",\"indrel_1mesIndex\",\n",
    "                         \"tiprel_1mesIndex\",\"indresiIndex\",\"indextIndex\", \n",
    "                         \"canal_entradaIndex\",\"nomprovIndex\",\"ind_actividad_clienteIndex\",\"segmentoIndex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = labels,\n",
    "                                outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "assemblerInputs = categoricalColumnsIDX + numericCols + targetsColumns\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs,\n",
    "                            outputCol = \"features\")\n",
    "stages += [assembler]  # Add stage to the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prePipeline = Pipeline(stages = stages)\n",
    "pipelineModel = prePipeline.fit(df_spark)\n",
    "\n",
    "dataset = pipelineModel.transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol = \"label\", \n",
    "                            featuresCol = \"features\", \n",
    "                            numTrees = 100,                 #  Number of trees in the random forest\n",
    "                            impurity = 'entropy',            # Criterion used for information gain calculation\n",
    "                            featureSubsetStrategy = \"auto\",\n",
    "                            predictionCol = \"prediction\",\n",
    "                            maxDepth = 5, \n",
    "                            maxBins = 160, \n",
    "                            minInstancesPerNode = 2) \n",
    "                            #minInfoGain=0.0, \n",
    "                            #subsamplingRate=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section C) Evaluation of Performance and testing\n",
    "Evaluate the performance of your pipeline using training and test set (don’t use CV but pyspark.ml.tuning.TrainValidationSplit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Evaluate performance of machine learning pipeline on training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports dependencies\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split data into training set and testing set\n",
    "[trainData, testData] = dataset.randomSplit([0.8, 0.2], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cross-validation\n",
      "finished cross-validation\n"
     ]
    }
   ],
   "source": [
    "# evaluation of model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", \n",
    "                                              predictionCol = \"prediction\", \n",
    "                                              metricName = \"accuracy\")\n",
    "# random forest parameters\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [100]).build()\n",
    "\n",
    "# cross-validation of model performance during grid-search \n",
    "# Method: pyspark.ml.tuning.TrainValidationSplit\n",
    "crossval = TrainValidationSplit(estimator = rf,\n",
    "                                estimatorParamMaps = paramGrid,\n",
    "                                evaluator = evaluator,\n",
    "                                trainRatio = 0.9)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "print('starting cross-validation')\n",
    "cvModel = crossval.fit(trainData)  # This takes time!\n",
    "print('finished cross-validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.989438\n",
      "Training Error = 0.0105624\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = cvModel.transform(trainData)\n",
    "train_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Training Accuracy = %g\" % (train_accuracy))\n",
    "print(\"Training Error = %g\" % (1.0 - train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.989246\n",
      "Test Error = 0.0107541\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = cvModel.transform(testData)\n",
    "test_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % (test_accuracy))\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Section D) Implement a parameter grid - Model fine-tuning \n",
    "\n",
    "### Note: This section takes long time to compute!\n",
    "\n",
    "Implement a parameter grid (using pyspark.ml.tuning.ParamGridBuilder[source]), varying at least one feature preprocessing step, one machine learning parameter, and the training set size. Document the training and test performance and the time taken for training and testing. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1. Evaluate model performance using a subset of preprocessing variables\n",
    "#### No numeric predictors used, relaunch pipeline with this new preprocessing structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# New preprocessing stage, without numeric predictors\n",
    "new_stages = []\n",
    "\n",
    "# remove preprocessing numeric predictors by including an empty vector\n",
    "New_numericCols = [] # empty numeric predictors\n",
    "\n",
    "# Add Newstages to the pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol,\n",
    "                                  outputCol = categoricalCol + \"Index\")\n",
    "    new_stages += [stringIndexer]  # Add stages to the pipeline\n",
    "\n",
    "new_stages += [label_stringIdx]\n",
    "\n",
    "# empty vector is inserted here\n",
    "new_assemblerInputs = categoricalColumnsIDX + New_numericCols + targetsColumns\n",
    "new_assembler = VectorAssembler(inputCols = new_assemblerInputs, outputCol = \"features\")\n",
    "\n",
    "new_stages += [new_assembler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating new pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "new_prePipeline = Pipeline(stages = new_stages)\n",
    "new_pipelineModel = new_prePipeline.fit(df_spark)\n",
    "\n",
    "new_dataset = new_pipelineModel.transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical predictors not used for training\n",
      "New Training Accuracy = 0.989438\n",
      "New Training Error = 0.0105624\n",
      "New Test Accuracy = 0.989246\n",
      "New Test Error = 0.0107541\n"
     ]
    }
   ],
   "source": [
    "[new_trainData, new_testData] = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "new_cvModel = crossval.fit(new_trainData)  # This takes time!\n",
    "\n",
    "# Results:\n",
    "print('Numerical predictors not used for training')\n",
    "\n",
    "new_predictions = cvModel.transform(new_trainData)\n",
    "new_train_accuracy = evaluator.evaluate(new_predictions)\n",
    "print(\"New Training Accuracy = %g\" % (new_train_accuracy))\n",
    "print(\"New Training Error = %g\" % (1.0 - new_train_accuracy))\n",
    "\n",
    "new_test_predictions = cvModel.transform(new_testData)\n",
    "new_test_accuracy = evaluator.evaluate(new_test_predictions)\n",
    "print(\"New Test Accuracy = %g\" % (new_test_accuracy))\n",
    "print(\"New Test Error = %g\" % (1.0 - new_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial products not used for training (only personal data)\n",
      "Training Accuracy = 0.989438\n",
      "Training Error = 0.0105624\n",
      "Test Accuracy = 0.989246\n",
      "Test Error = 0.0107541\n"
     ]
    }
   ],
   "source": [
    "print('Financial products not used for training (only personal data)')\n",
    "\n",
    "# Add Newstages to the pipeline\n",
    "stages2 = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol,\n",
    "                                  outputCol = categoricalCol + \"Index\")\n",
    "    stages2 += [stringIndexer]  # Add stages to the pipeline\n",
    "\n",
    "stages2 += [label_stringIdx]\n",
    "\n",
    "# empty vector is inserted here\n",
    "new_assemblerInputs = categoricalColumnsIDX + numericCols\n",
    "new_assembler = VectorAssembler(inputCols = new_assemblerInputs, outputCol = \"features\")\n",
    "\n",
    "stages2 += [new_assembler]\n",
    "\n",
    "# Creating new pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "prePipeline2 = Pipeline(stages = stages2)\n",
    "pipelineModel2 = prePipeline2.fit(df_spark)\n",
    "\n",
    "dataset2 = pipelineModel2.transform(df_spark)\n",
    "\n",
    "[trainData2, testData2] = dataset2.randomSplit([0.8, 0.2], seed = 100)\n",
    "\n",
    "cvModel2 = crossval.fit(trainData2)  # This takes time!\n",
    "\n",
    "# Results:\n",
    "predictions2 = cvModel2.transform(trainData2)\n",
    "train_accuracy2 = evaluator.evaluate(predictions2)\n",
    "print(\"Training Accuracy = %g\" % (train_accuracy2))\n",
    "print(\"Training Error = %g\" % (1.0 - train_accuracy2))\n",
    "\n",
    "test_predictions2 = cvModel2.transform(testData2)\n",
    "test_accuracy2 = evaluator.evaluate(test_predictions2)\n",
    "print(\"Test Accuracy = %g\" % (test_accuracy2))\n",
    "print(\"Test Error = %g\" % (1.0 - test_accuracy2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2. Training set size evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training set size evaluation')\n",
    "\n",
    "%time\n",
    "\n",
    "# size of different training set to be evaluated, and split of training set\n",
    "sizes = [0.1, 0.001, 0.00001]\n",
    "data = trainData.randomSplit(sizes, seed = 100)\n",
    "\n",
    "# model performance with full dataset, from previous experiment\n",
    "print('\\n\\n=== training set of size 100%:')\n",
    "print(\"Classification Error = %g\" % (1.0 - new_train_accuracy))\n",
    "\n",
    "i = 0\n",
    "for split in data:\n",
    "    print('\\n\\n=== training set of size reduced to {}%, wait please'.format(sizes[i]*100))\n",
    "    cvModel = crossval.fit(split)\n",
    "    predictions = cvModel.transform(split)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Classification Error = %g\" % (1.0 - accuracy))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3. Machine Learning Model Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and their values to search and evaluate\n",
    "%time\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,100,500]) \\\n",
    "    .addGrid(rf.maxDepth, [2,10]).build()\n",
    "\n",
    "# cross-validation of model performance during grid-search \n",
    "crossval = TrainValidationSplit(estimator = rf,\n",
    "                                estimatorParamMaps = paramGrid,\n",
    "                                evaluator = evaluator,\n",
    "                                trainRatio = 0.9)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "print('starting Hyperparameter Grid Search with cross-validation')\n",
    "cvModel = crossval.fit(trainData)\n",
    "print('Grid Search with cross-validation has finished')\n",
    "\n",
    "# pick best model\n",
    "rfModel = cvModel.bestModel\n",
    "print (rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for test set and compute test error\n",
    "predictions = rfModel.transform(testData)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Findings and conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As expected, Random Forest is able to return a good generalisation accuracy (validated on the Test Data). The average classification accuracy on the Test Data across different models tested is higher than 98%. The level of abstraction and flexibility offer by the RF model allows for a excellent accuracy results even when the number of predictors is reduced, e.i. not using existing financial products as predictors, or removing numerical predictors from the training set (as seen in section 4.1). This process resulted in a simpler model yet equally powerful.\n",
    "\n",
    "As expected, the model is sensitive to the reduction in the size of the training data set. The more data the better the model. More training data reduces the effect of outliers and increases the generalisation accuracy of the final model. Besides, more data is likely to reduce the effect of bias in the data. As seen in section 4.2, the accuracy can vary widely depending on randomness selecting training/validation data. Less data makes more likely selecting a no representative sample for training the model, or for the evaluation. \n",
    "\n",
    "The grid search performed as part of this analysis shows that a RF containing more small trees (low depth) provided better results with less computational cost than a smaller number of deeper trees. Depth trees increase computational cost exponentially and have greater risk of overfitting, without a significant gain in performance (section 4.3).\n",
    "\n",
    "In summary, Random Forest is a valid approach to perform classification predictions given the nature of the data, large, complex, non-linear and non-uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
